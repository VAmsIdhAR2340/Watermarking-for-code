{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import hashlib\n",
        "import scipy.stats\n",
        "from math import sqrt\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "class WatermarkForCode:\n",
        "    def __init__(self, model, tokenizer, entropy_threshold, gamma, delta):\n",
        "        \"\"\"\n",
        "        Initialize the SWEET (Selective WatErmarking via Entropy Thresholding) watermarker.\n",
        "\n",
        "        Args:\n",
        "        - model: The language model used for text generation\n",
        "        - tokenizer: The tokenizer corresponding to the model\n",
        "        - entropy_threshold: Threshold for high-entropy tokens (τ in the algorithm)\n",
        "        - gamma: Proportion of vocabulary to be considered as \"green\" tokens (γ in the algorithm)\n",
        "        - delta: Logit increase for green tokens (δ in the algorithm)\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.entropy_threshold = entropy_threshold\n",
        "        self.gamma = gamma\n",
        "        self.delta = delta\n",
        "\n",
        "    def calculate_entropy(self, probs):\n",
        "        \"\"\"Calculate the entropy of a probability distribution.\"\"\"\n",
        "        return -torch.sum(probs * torch.log2(probs + 1e-10))\n",
        "\n",
        "    def generate_with_watermark(self, prompt, max_length=200):\n",
        "        \"\"\"\n",
        "        Generate text with SWEET watermarking applied.\n",
        "\n",
        "        This method implements Algorithm 1 from the SWEET paper.\n",
        "        \"\"\"\n",
        "\n",
        "        input_ids = self.tokenizer.encode(prompt, return_tensors='pt')\n",
        "        generated_ids = input_ids.clone()\n",
        "        watermarked_tokens = []\n",
        "        past = None\n",
        "\n",
        "\n",
        "        for t in range(max_length):\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(input_ids=input_ids, past_key_values=past)\n",
        "                logits = outputs.logits[:, -1, :]\n",
        "                past = outputs.past_key_values\n",
        "\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "\n",
        "            entropy = self.calculate_entropy(probs)\n",
        "\n",
        "\n",
        "            if entropy > self.entropy_threshold:\n",
        "\n",
        "                prev_token = generated_ids[0, -1].item()\n",
        "                hash_value = hashlib.sha256(str(prev_token).encode()).hexdigest()\n",
        "                np.random.seed(int(hash_value, 16) % (2**32))\n",
        "\n",
        "\n",
        "                vocab_size = logits.shape[-1]\n",
        "                green_size = int(self.gamma * vocab_size)\n",
        "                green_indices = np.random.choice(vocab_size, green_size, replace=False)\n",
        "\n",
        "\n",
        "                logits[0, green_indices] += self.delta\n",
        "                watermarked_tokens.append(True)\n",
        "            else:\n",
        "                watermarked_tokens.append(False)\n",
        "\n",
        "\n",
        "            next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
        "\n",
        "            # Append to generated sequence\n",
        "            generated_ids = torch.cat([generated_ids, next_token], dim=-1)\n",
        "            input_ids = next_token\n",
        "\n",
        "            if next_token.item() == self.tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "        generated_text = self.tokenizer.decode(generated_ids[0])\n",
        "        generated_tokens = self.tokenizer.convert_ids_to_tokens(generated_ids[0])\n",
        "\n",
        "        return generated_text, generated_tokens, watermarked_tokens\n",
        "\n",
        "    def detect_watermark(self, text, n_grams=2):\n",
        "        \"\"\"\n",
        "        Detect the presence of a watermark in the given text.\n",
        "\n",
        "        This method implements the watermark detection algorithm described in the SWEET paper.\n",
        "        \"\"\"\n",
        "        # Tokenize the input text\n",
        "        tokens = self.tokenizer.encode(text)\n",
        "        green_counts = []\n",
        "\n",
        "        # Iterate through n-grams in the text\n",
        "        for i in range(len(tokens) - n_grams):\n",
        "            # Extract the current n-gram\n",
        "            n_gram = tokens[i:i+n_grams]\n",
        "\n",
        "            # Compute hash of the n-gram to seed the random number generator\n",
        "            hash_value = hashlib.sha256(str(n_gram).encode()).hexdigest()\n",
        "            np.random.seed(int(hash_value, 16) % (2**32))\n",
        "\n",
        "            # Randomly select green tokens based on gamma\n",
        "            vocab_size = len(self.tokenizer)\n",
        "            green_size = int(self.gamma * vocab_size)\n",
        "            green_set = set(np.random.choice(vocab_size, green_size, replace=False))\n",
        "\n",
        "            # Check if the next token is in the green set\n",
        "            next_token = tokens[i + n_grams]\n",
        "            if next_token in green_set:\n",
        "                green_counts.append(1)\n",
        "            else:\n",
        "                green_counts.append(0)\n",
        "\n",
        "        # Calculate observed green ratio (observed watermark fraction)\n",
        "        observed_wl_frac = np.mean(green_counts)\n",
        "\n",
        "        # Total number of tokens checked\n",
        "        T = len(green_counts)\n",
        "\n",
        "        # Compute z-score using the provided formula\n",
        "        def compute_z_score(observed_wl_frac, T, gamma):\n",
        "            numer = observed_wl_frac - gamma\n",
        "            denom = sqrt(gamma * (1 - gamma) / T)\n",
        "            z = numer / denom\n",
        "            return z\n",
        "\n",
        "        z_score = compute_z_score(observed_wl_frac, T, self.gamma)\n",
        "\n",
        "        # Compute p-value using the survival function of the standard normal distribution\n",
        "        p_value = scipy.stats.norm.sf(abs(z_score))\n",
        "\n",
        "        return z_score, p_value, observed_wl_frac, self.gamma\n",
        "\n",
        "    def interpret_watermark_detection(self, z_score, p_value, observed_ratio, expected_ratio, significance_level=0.05):\n",
        "        \"\"\"\n",
        "        Interpret the results of watermark detection.\n",
        "\n",
        "        This method provides a human-readable interpretation of the statistical results.\n",
        "        \"\"\"\n",
        "        if observed_ratio > expected_ratio or p_value < significance_level:\n",
        "            return f\"The text is likely watermarked (z-score: {z_score:.4f}, p-value: {p_value:.4f}, observed ratio: {observed_ratio:.4f}, expected ratio: {expected_ratio:.4f})\"\n",
        "        else:\n",
        "            return f\"The text is not conclusively watermarked (z-score: {z_score:.4f}, p-value: {p_value:.4f}, observed ratio: {observed_ratio:.4f}, expected ratio: {expected_ratio:.4f})\"\n",
        "\n",
        "# Example usage\n",
        "model_name = \"Salesforce/codegen-350M-mono\"  # Code-specific model\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Initialize SWEET watermarker with parameters tuned for code generation\n",
        "watermarker = WatermarkForCode(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    entropy_threshold=3.0,  # Entropy threshold τ for selective watermarking\n",
        "    gamma=0.4,  # Proportion γ of vocabulary to be considered as green tokens\n",
        "    delta=0.1   # Logit increase δ for green tokens\n",
        ")\n",
        "\n",
        "# Generate watermarked code\n",
        "prompt = \"def factorial(n):\\n    # Implement Factorial function\\n    \"\n",
        "generated_text, generated_tokens, watermarked_tokens = watermarker.generate_with_watermark(prompt, max_length=200)\n",
        "\n",
        "print(\"Prompt:\")\n",
        "print(prompt)\n",
        "print(\"\\nGenerated Text:\")\n",
        "print(generated_text)\n",
        "print(\"\\nWatermarking Statistics:\")\n",
        "print(f\"{sum(watermarked_tokens)} out of {len(watermarked_tokens)} tokens watermarked\")\n",
        "print(f\"Watermarking rate: {sum(watermarked_tokens)/len(watermarked_tokens):.2%}\")\n",
        "\n",
        "print(\"\\nWatermarked Tokens:\")\n",
        "for token, is_watermarked in zip(generated_tokens[len(tokenizer.encode(prompt)):], watermarked_tokens):\n",
        "    if is_watermarked:\n",
        "        print(tokenizer.convert_tokens_to_string([token]))\n",
        "\n",
        "# Detect and interpret watermark\n",
        "z_score, p_value, observed_ratio, expected_ratio = watermarker.detect_watermark(generated_text)\n",
        "print(f\"\\nWatermark Detection:\")\n",
        "print(watermarker.interpret_watermark_detection(z_score, p_value, observed_ratio, expected_ratio))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDs8o0UvwmVf",
        "outputId": "9f0c6d06-ecd0-4d76-f438-f7c9ae22a5ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt:\n",
            "def factorial(n):\n",
            "    # Implement Factorial function\n",
            "    \n",
            "\n",
            "Generated Text:\n",
            "def factorial(n):\n",
            "    # Implement Factorial function\n",
            "    # Be careful\n",
            "    if n<0:\n",
            "        raise ValueError\n",
            "\n",
            "  \n",
            "    \n",
            "    \n",
            "\n",
            "def is_palindrome(number):\n",
            "    is_positive = True\n",
            "    number = str(number)\n",
            "        \n",
            "    ### Initializing the loop variables i,j where i is a negative number\n",
            "    # We always begin it with 0 at the first iteration\n",
            "    # i = 0\n",
            "    i = 0\n",
            "    j = 0\n",
            "    \n",
            "          \n",
            "    # The condition corresponding to 4 without $ signs\n",
            "    if len(number)==1: \n",
            "        is_one = True\n",
            "    else:\n",
            "          \n",
            "        j=len(number)-1\n",
            "          \n",
            "        while j>=0 or is_positive:\n",
            "            is_number = True\n",
            "            if j>=0 and is_negative and number[j]=='$':\n",
            "                number = number[0:j]+number[j+1:]\n",
            "\n",
            "Watermarking Statistics:\n",
            "49 out of 200 tokens watermarked\n",
            "Watermarking rate: 24.50%\n",
            "\n",
            "Watermarked Tokens:\n",
            "#\n",
            " Be\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "def\n",
            " is\n",
            "pal\n",
            "number\n",
            "number\n",
            "###\n",
            " Initial\n",
            "izing\n",
            " the\n",
            " loop\n",
            " variables\n",
            " where\n",
            " i\n",
            " is\n",
            " a\n",
            " negative\n",
            " number\n",
            "\n",
            "\n",
            " We\n",
            " always\n",
            " begin\n",
            " it\n",
            " with\n",
            " 0\n",
            " at\n",
            " the\n",
            " first\n",
            " iteration\n",
            "\n",
            "\n",
            " i\n",
            " =\n",
            "\n",
            "\n",
            " The\n",
            " condition\n",
            " corresponding\n",
            " 4\n",
            " without\n",
            " $\n",
            " signs\n",
            "\n",
            "\n",
            "==\n",
            "len\n",
            "if\n",
            "\n",
            "Watermark Detection:\n",
            "The text is likely watermarked (z-score: 0.5315, p-value: 0.2975, observed ratio: 0.4178, expected ratio: 0.4000)\n"
          ]
        }
      ]
    }
  ]
}